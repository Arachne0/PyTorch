{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOFEGVqjCzrMWaCRKJJA8Gt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arachne0/PyTorch/blob/master/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "id": "m1A9wraPT6Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YfeJk6MnDBI"
      },
      "outputs": [],
      "source": [
        "#@title Patch Embedding\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(8, 3, 224, 224,device = \"cuda\")\n",
        "x.shape"
      ],
      "metadata": {
        "id": "9GVECnh8EwZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size = 16 # 16 pixels\n",
        "\n",
        "print('x :', x.shape)\n",
        "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
        "# Batch * C * H ( W 형태를 가진 이미지를 Batch * N * (P * P * C) 의 벡터로 임베딩해주어야 한다. P: batch_size, N )\n",
        "print('patches :', patches.shape)"
      ],
      "metadata": {
        "id": "ncQsBDGJE4BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size = 16\n",
        "in_channels = 3\n",
        "emb_size = 768\n",
        "\n",
        "projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "projection(x).shape"
      ],
      "metadata": {
        "id": "pDEZ8UMqFNDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_size = 768\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "\n",
        "# 이미지를 패치사이즈로 나누고 flatten\n",
        "projected_x = projection(x)\n",
        "print('Projected X shape :', projected_x.shape)\n",
        "\n",
        "# cls_token과 pos encoding Parameter 정의\n",
        "cls_token = nn.Parameter(torch.randn(1,1, emb_size)).to(\"cuda\")\n",
        "positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size)).to(\"cuda\")\n",
        "print('Cls Shape :', cls_token.shape, ', Pos Shape :', positions.shape)\n",
        "\n",
        "# cls_token을 반복하여 배치사이즈의 크기와 맞춰줌\n",
        "batch_size = 8\n",
        "cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
        "print('Repeated Cls shape :', cls_tokens.shape)\n",
        "\n",
        "# cls_token과 projected_x를 concatenate\n",
        "cat_x = torch.cat([cls_tokens, projected_x], dim=1).to(\"cuda\")\n",
        "\n",
        "# position encoding을 더해줌\n",
        "cat_x += positions\n",
        "print('output : ', cat_x.shape)"
      ],
      "metadata": {
        "id": "SiP10aiTH7t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        ).to(\"cuda\")\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size)).to(\"cuda\")\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size)).to(\"cuda\")\n",
        "        \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1).cuda()\n",
        "        # add position embedding\n",
        "        x += self.positions\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ViIk4K2VILYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch = PatchEmbedding()\n",
        "x = patch.forward(x)"
      ],
      "metadata": {
        "id": "c6rIXLJaINth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Linear Projection\n",
        "emb_size = 768\n",
        "num_heads = 8\n",
        "queries= nn.Linear(emb_size, emb_size).cuda()\n",
        "keys = nn.Linear(emb_size, emb_size).cuda()\n",
        "values = nn.Linear(emb_size, emb_size).cuda()\n",
        "# ViT에서는 QKV가 같은 Tensor로 입력된다.\n",
        "# input Tensor는 3개의 Linear Projection을 통해 여러개의 Head로 나눠진후 각각 Sclaed Dot-Product Attention을 한다\n",
        "print(keys, queries, values)"
      ],
      "metadata": {
        "id": "CrKWVqylIfXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Multi Head\n",
        "queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
        "keys = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
        "values  = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
        "# Linear Projection을 통해 구한 QKV를 reagrrange 을 통해 8개의 Multi-Head로 나눠준다\n",
        "\n",
        "print('shape :', queries.shape, keys.shape, values.shape)"
      ],
      "metadata": {
        "id": "evPq9FNyIjAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sclae Dot-Product Attention\n",
        "# Queries * Keys\n",
        "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys).cuda()\n",
        "print('energy :', energy.shape)\n",
        "\n",
        "# Get Attention Score\n",
        "scaling = emb_size ** (1/2)\n",
        "att = F.softmax(energy, dim=-1) / scaling\n",
        "print('att :', att.shape)\n",
        "\n",
        "# Attention Score * values\n",
        "out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "print('out :', out.shape)\n",
        "\n",
        "# Rearrage to emb_size\n",
        "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "print('out2 : ', out.shape)"
      ],
      "metadata": {
        "id": "86ijYq1bN5-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3).cuda()\n",
        "        self.att_drop = nn.Dropout(dropout).cuda()\n",
        "        self.projection = nn.Linear(emb_size, emb_size).cuda()\n",
        "        \n",
        "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
        "        # split keys, queries and values in num_heads\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        # sum up over the last axis\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "            \n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "hFu5LpHJVYmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Resudual Block\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x"
      ],
      "metadata": {
        "id": "Kle6SqcCVasz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title FeedForward MLP\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size).cuda(),\n",
        "            nn.GELU().cuda(),\n",
        "            nn.Dropout(drop_p).cuda(),\n",
        "            nn.Linear(expansion * emb_size, emb_size).cuda(),\n",
        "        )\n",
        "        # 첫번째 레이어에서는 expansion을 곱해준 만큼 임베딩 사이즈를 확장하고, \n",
        "        # GELU와 Dropout 후에 두번째 Linear 레이어에서 다시 원래의 emb_size로 축소한다."
      ],
      "metadata": {
        "id": "KAA6i8ywVclR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Transformer Encoder Block\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size: int = 768,\n",
        "                 drop_p: float = 0.,\n",
        "                 forward_expansion: int = 4,\n",
        "                 forward_drop_p: float = 0.,\n",
        "                 ** kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size).cuda(),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p).cuda()\n",
        "            ).cuda()),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size).cuda(),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p).cuda()\n",
        "            ).cuda()\n",
        "            ))"
      ],
      "metadata": {
        "id": "fznRq87UVgLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Building Block\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth: int = 12, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
        "        # *이 붙은 이유는 인자를 list형식으로 보내는 것이 아니라 각각 나눠서 보내줘야하기 때문"
      ],
      "metadata": {
        "id": "c1Aw3cwI9Vpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Head\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size: int = 768, n_classes: int = 10):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size).cuda(), \n",
        "            nn.Linear(emb_size, n_classes).cuda())"
      ],
      "metadata": {
        "id": "suHysweiVm4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Summary\n",
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,     \n",
        "                in_channels: int = 3,\n",
        "                patch_size: int = 16,\n",
        "                emb_size: int = 768,\n",
        "                img_size: int = 224,\n",
        "                depth: int = 12,\n",
        "                n_classes: int = 10,\n",
        "                **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )\n",
        "        \n",
        "summary(ViT(), (3, 224, 224), device='cuda')"
      ],
      "metadata": {
        "id": "eg7CY4cUVm5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT()"
      ],
      "metadata": {
        "id": "vxeoanIxVm71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "IEayDe-f32uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "AGY9pOY8Vm9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "mplVbGlLVm_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "      transforms.Resize(224)])"
      ],
      "metadata": {
        "id": "FzjqFyKZXOAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform\n",
        "                                        )\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "g4dUoHRPXP4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10 # 원래는 100인데 size error 나서 10으로 줄임 따사서 accuracy는 낮을 수 밖에 \n",
        "num_classes = 10\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "IVUm1mEvXRLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "lcQCVd01XUaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "nnEyANDsXUbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8MO6m0aU1zyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs): # epochs수만큼 반복\n",
        "    avg_cost = 0\n",
        "\n",
        "    for data, target in tqdm(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad() # 모든 model의 gradient 값을 0으로 설정\n",
        "        hypothesis = model(data) # 모델을 forward pass해 결과값 저장 \n",
        "        cost = criterion(hypothesis, target) # output과 target의 loss 계산\n",
        "        cost.backward() # backward 함수를 호출해 gradient 계산\n",
        "        optimizer.step() # 모델의 학습 파라미터 갱신\n",
        "        avg_cost += cost / len(train_loader) # loss 값을 변수에 누적하고 train_loader의 개수로 나눔 = 평균\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
      ],
      "metadata": {
        "id": "VzUqXhK7XUdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # evaluate mode로 전환 dropout 이나 batch_normalization 해제 \n",
        "with torch.no_grad(): # grad 해제 \n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in tqdm(test_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        out = model(data)\n",
        "        preds = torch.max(out.data, 1)[1] # 출력이 분류 각각에 대한 값으로 나타나기 때문에, 가장 높은 값을 갖는 인덱스를 추출\n",
        "        total += len(target) # 전체 클래스 개수 \n",
        "        correct += (preds==target).sum().item() # 예측값과 실제값이 같은지 비교\n",
        "        \n",
        "    print('Test Accuracy: ', 100.*correct/total, '%')\n"
      ],
      "metadata": {
        "id": "qto_s_5gXUnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "uqIlbp1XXUp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}